{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PearlSikka/SentimentAnalysis_Twitter/blob/main/SentimentAnalysis_Twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSKnrUEumQxT"
      },
      "source": [
        "# Sentiment Classification\n",
        "\n",
        "Classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqtl05vRmQxg"
      },
      "source": [
        "**IMPORT necessary packages**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI58Ej-UmQxi",
        "outputId": "5dfc59a9-1ed9-479c-b518-4f9a8d0c38f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "import re\n",
        "import string\n",
        "from os.path import join\n",
        "import numpy as np                                                                #for array manipulations\n",
        "import pandas as pd                                                               #for dataframe manipulations\n",
        "import seaborn as sns                                                             #for plots\n",
        "import nltk                                                                       #to process and manipulate text data\n",
        "from nltk import PorterStemmer\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop=set(stopwords.words('english'))\n",
        "stemmer=PorterStemmer()\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer       #for feature extraction\n",
        "from sklearn import svm                                                           #SVM\n",
        "from sklearn.naive_bayes import MultinomialNB                                     #Naive Bayes\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier                               #RandomForest\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score,f1_score   \n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer                                    #Keras Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences                            \n",
        "\n",
        "import torch                                                                      #pytorch \n",
        "from torch import nn                                                              #pytorch neural network building class\n",
        "from torch.utils import data\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o079KIVruCbo"
      },
      "source": [
        "**LOAD DATASET: Loading training, validation/dev and test datasets.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8C7JBYah4SY"
      },
      "outputs": [],
      "source": [
        "# Define test sets\n",
        "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']      #testsets. *Please add the additional testsets here*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "AA7_SCUht-XN",
        "outputId": "771290ab-8a96-44f4-cb1d-aceb46648d8e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3e1a7f93-47e3-41f1-a69b-3db0f51272e7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>335104872099066692</th>\n",
              "      <td>positive</td>\n",
              "      <td>Felt privileged to play Foo Fighters songs on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796528524030124618</th>\n",
              "      <td>positive</td>\n",
              "      <td>\"@AaqibAfzaal Pakistan may be an Islamic count...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>760964834217238632</th>\n",
              "      <td>positive</td>\n",
              "      <td>Happy Birthday to the coolest golfer in Bali! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147713180324524046</th>\n",
              "      <td>negative</td>\n",
              "      <td>@SimpplyA TMILLS is going to Tucson! But the 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>732302280474120023</th>\n",
              "      <td>negative</td>\n",
              "      <td>Hmmmmm where are the #BlackLivesMatter when ma...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e1a7f93-47e3-41f1-a69b-3db0f51272e7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3e1a7f93-47e3-41f1-a69b-3db0f51272e7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3e1a7f93-47e3-41f1-a69b-3db0f51272e7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                   sentiment                                              tweet\n",
              "id                                                                             \n",
              "335104872099066692  positive  Felt privileged to play Foo Fighters songs on ...\n",
              "796528524030124618  positive  \"@AaqibAfzaal Pakistan may be an Islamic count...\n",
              "760964834217238632  positive  Happy Birthday to the coolest golfer in Bali! ...\n",
              "147713180324524046  negative  @SimpplyA TMILLS is going to Tucson! But the 2...\n",
              "732302280474120023  negative  Hmmmmm where are the #BlackLivesMatter when ma..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datadir= '/content/semeval-tweets/'                                             #load data directory. *Please change according to the location of data*\n",
        "\n",
        "df_test={}                                                        \n",
        "\n",
        "data_df_train=pd.read_csv(datadir +'twitter-training-data.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE, error_bad_lines=False)   #reading training dataset\n",
        "data_df_dev=pd.read_csv(datadir +'twitter-dev-data.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE, error_bad_lines=False)          #reading dev dataset\n",
        "\n",
        "data_df_train.rename(columns={0:'id',1:'sentiment',2:'tweet'},inplace=True)                                  #to give columns meaningful name\n",
        "data_df_dev.rename(columns={0:'id',1:'sentiment',2:'tweet'},inplace=True)                             \n",
        "\n",
        "data_df_train=data_df_train.set_index('id')                                                                    \n",
        "data_df_dev=data_df_dev.set_index('id')\n",
        "\n",
        "k=0\n",
        "for test in testsets:                                                                                        #reading testsets\n",
        "  k=k+1\n",
        "  df_test['data_df_test'+str(k)]= pd.read_csv(datadir + test, sep='\\t', header=None, quoting=csv.QUOTE_NONE, error_bad_lines=False)\n",
        "  df_test['data_df_test'+str(k)].rename(columns={0:'id',1:'sentiment',2:'tweet'},inplace=True)\n",
        "  df_test['data_df_test'+str(k)]=df_test['data_df_test'+str(k)].set_index('id')\n",
        "\n",
        "data_df_train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0OAdRSaTy-R",
        "outputId": "e1882086-f101-4cd8-db76-4746de46978c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 45101 tweets in training set\n",
            "There are 2000 tweets in dev set\n",
            "There are 3531 tweets in testset data_df_test1\n",
            "There are 1853 tweets in testset data_df_test2\n",
            "There are 2379 tweets in testset data_df_test3\n"
          ]
        }
      ],
      "source": [
        "print(f'There are {data_df_train.shape[0]} tweets in training set')\n",
        "print(f'There are {data_df_dev.shape[0]} tweets in dev set')\n",
        "\n",
        "for i in df_test:\n",
        "  print(f'There are {df_test[i].shape[0]} tweets in testset {i}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVpicfB4Ylin",
        "outputId": "a2ba9f59-36fa-445f-e6af-1a33c5592864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing sample in training set: False\n",
            "Missing sample in development set: False\n",
            "Missing sample in test set: False\n",
            "Missing sample in test set: False\n",
            "Missing sample in test set: False\n"
          ]
        }
      ],
      "source": [
        "print(f\"Missing sample in training set: {data_df_train.isnull().values.any()}\")\n",
        "print(f\"Missing sample in development set: {data_df_dev.isnull().values.any()}\")\n",
        "\n",
        "for i in df_test:\n",
        "  print(f\"Missing sample in test set: {df_test[i].isnull().values.any()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W8euQbiSHn7"
      },
      "source": [
        "**CLASS DISTRIBUTION**: Before starting with analysis, it's important to see the distribution of values in classes in training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "qpbrABDCSLEd",
        "outputId": "d3ea6002-60b4-4f47-bf56-d177f696d636"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6646483ed0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.countplot(x='sentiment', data=data_df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIRcv11lyz7k"
      },
      "source": [
        "**PREPROCESS TWEETS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJnaLyJxywjU"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    text = re.sub(r\"he's\", \"he is\", text)                                        #removing contractions\n",
        "    text = re.sub(r\"there's\", \"there is\", text)\n",
        "    text = re.sub(r\"We're\", \"We are\", text)\n",
        "    text = re.sub(r\"That's\", \"That is\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"they're\", \"they are\", text)\n",
        "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
        "    text = re.sub(r\"Can't\", \"Cannot\", text)\n",
        "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
        "    text = re.sub(r\"don\\x89Ûªt\", \"do not\", text)\n",
        "    text = re.sub(r\"aren't\", \"are not\", text)\n",
        "    text = re.sub(r\"isn't\", \"is not\", text)\n",
        "    text = re.sub(r\"What's\", \"What is\", text)\n",
        "    text = re.sub(r\"haven't\", \"have not\", text)\n",
        "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
        "    text = re.sub(r\"There's\", \"There is\", text)\n",
        "    text = re.sub(r\"He's\", \"He is\", text)\n",
        "    text = re.sub(r\"It's\", \"It is\", text)\n",
        "    text = re.sub(r\"You're\", \"You are\", text)\n",
        "    text = re.sub(r\"I'M\", \"I am\", text)\n",
        "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
        "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
        "    text = re.sub(r\"i'm\", \"I am\", text)\n",
        "    text = re.sub(r\"I\\x89Ûªm\", \"I am\", text)\n",
        "    text = re.sub(r\"I'm\", \"I am\", text)\n",
        "    text = re.sub(r\"Isn't\", \"is not\", text)\n",
        "    text = re.sub(r\"Here's\", \"Here is\", text)\n",
        "    text = re.sub(r\"you've\", \"you have\", text)\n",
        "    text = re.sub(r\"you\\x89Ûªve\", \"you have\", text)\n",
        "    text = re.sub(r\"we're\", \"we are\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
        "    text = re.sub(r\"we've\", \"we have\", text)\n",
        "    text = re.sub(r\"it\\x89Ûªs\", \"it is\", text)\n",
        "    text = re.sub(r\"doesn\\x89Ûªt\", \"does not\", text)\n",
        "    text = re.sub(r\"It\\x89Ûªs\", \"It is\", text)\n",
        "    text = re.sub(r\"Here\\x89Ûªs\", \"Here is\", text)\n",
        "    text = re.sub(r\"who's\", \"who is\", text)\n",
        "    text = re.sub(r\"I\\x89Ûªve\", \"I have\", text)\n",
        "    text = re.sub(r\"y'all\", \"you all\", text)\n",
        "    text = re.sub(r\"can\\x89Ûªt\", \"cannot\", text)\n",
        "    text = re.sub(r\"would've\", \"would have\", text)\n",
        "    text = re.sub(r\"it'll\", \"it will\", text)\n",
        "    text = re.sub(r\"we'll\", \"we will\", text)\n",
        "    text = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", text)\n",
        "    text = re.sub(r\"We've\", \"We have\", text)\n",
        "    text = re.sub(r\"he'll\", \"he will\", text)\n",
        "    text = re.sub(r\"Y'all\", \"You all\", text)\n",
        "    text = re.sub(r\"Weren't\", \"Were not\", text)\n",
        "    text = re.sub(r\"Didn't\", \"Did not\", text)\n",
        "    text = re.sub(r\"they'll\", \"they will\", text)\n",
        "    text = re.sub(r\"they'd\", \"they would\", text)\n",
        "    text = re.sub(r\"DON'T\", \"DO NOT\", text)\n",
        "    text = re.sub(r\"That\\x89Ûªs\", \"That is\", text)\n",
        "    text = re.sub(r\"they've\", \"they have\", text)\n",
        "    text = re.sub(r\"i'll\", \"I will\", text)\n",
        "    text = re.sub(r\"weren't\", \"were not\", text)\n",
        "    text = re.sub(r\"They're\", \"They are\", text)\n",
        "    text = re.sub(r\"let's\", \"let us\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"don't\", \"do not\", text)\n",
        "    text = re.sub(r\"you're\", \"you are\", text)\n",
        "    text = re.sub(r\"i've\", \"I have\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"i'll\", \"I will\", text)\n",
        "    text = re.sub(r\"doesn't\", \"does not\", text)\n",
        "    text = re.sub(r\"i'd\", \"I would\", text)\n",
        "    text = re.sub(r\"didn't\", \"did not\", text)\n",
        "    text = re.sub(r\"ain't\", \"am not\", text)\n",
        "    text = re.sub(r\"you'll\", \"you will\", text)\n",
        "    text = re.sub(r\"I've\", \"I have\", text)\n",
        "    text = re.sub(r\"Don't\", \"do not\", text)\n",
        "    text = re.sub(r\"I'll\", \"I will\", text)\n",
        "    text = re.sub(r\"I'd\", \"I would\", text)\n",
        "    text = re.sub(r\"Let's\", \"Let us\", text)\n",
        "    text = re.sub(r\"you'd\", \"You would\", text)\n",
        "    text = re.sub(r\"It's\", \"It is\", text)\n",
        "    text = re.sub(r\"Ain't\", \"am not\", text)\n",
        "    text = re.sub(r\"Haven't\", \"Have not\", text)\n",
        "    text = re.sub(r\"Could've\", \"Could have\", text)\n",
        "    text = re.sub(r\"youve\", \"you have\", text)  \n",
        "    text = re.sub(r\"I'm\", \"I am\", text) \n",
        "    text = re.sub(r\"youll\", \"you will\", text)\n",
        "    text = re.sub(r\"wont\", \"will not\", text)\n",
        "    text = re.sub(r\"cant\", \"can not\", text)\n",
        "    text = re.sub(r\"dont\", \"do not\", text)\n",
        "\n",
        "    text=  re.sub(r'^RT[\\s]+', '', text)                                        #substituting for retweet RT \n",
        "    text = str(text).lower()                                                    #lowercase characters \n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)                            #substituting URLs \n",
        "    text = re.sub('\\[.*?\\]', '', text)                                          #substituting for []\n",
        "    text = re.sub('\\n', '', text)                                               #substituting new line  \n",
        "    text = re.sub('\\w*\\d\\w*', '', text)                                         #substituting digits  \n",
        "    text = re.sub('<.:*?>+', '', text)                                          #substituting for <>\n",
        "    text=  re.sub(r'#', '', text)                                               #substituting for hashtag. Not removing the entire hashtag word\n",
        "    text=  re.sub('@\\w*','',text)                                               #substituting for @\n",
        "    text = re.sub(r'\\s+', ' ', text)                                            #substituting multiple spaces with one space\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)            #removing punctuations\n",
        "                  \n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5pEhFJ-9CYb"
      },
      "outputs": [],
      "source": [
        "def deEmojify(text):                                                                  \n",
        "  re_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "  \n",
        "  return re_pattern.sub(r'',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ndD4oQORiVY"
      },
      "outputs": [],
      "source": [
        "def stem_words(text):                      \n",
        "    split_word=text.split()                           \n",
        "    cleaned_word=\" \".join([stemmer.stem(i) for i in split_word])                   #to stem the words\n",
        "    cleaned_word= re.sub(r'\\b\\w{1}\\b','',cleaned_word)                              #to remove words of length 1 \n",
        "    return cleaned_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU_w5BZ6rABq"
      },
      "outputs": [],
      "source": [
        "embed_size = 100                                                                  # how big is each word vector\n",
        "max_features = 5000                                                               # how many unique words to use (i.e num rows in embedding vector)\n",
        "max_len = 100                                                                     # max number of words in a question to use\n",
        "batch_size = 1024                                                                 # how many samples to process at once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVRxaQ3_bRl3"
      },
      "source": [
        "**FEATURE SELECTION AND ENGINEERING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBxnjt-8Y5c2"
      },
      "outputs": [],
      "source": [
        "#feature extraction using TF-IDF \n",
        "def tf_feature_eng():\n",
        "  tf=TfidfVectorizer(min_df=1, max_df=0.9, ngram_range=(1,3), analyzer='word', max_features=5000)       #creating unigram and bigrams      \n",
        "  tf.fit(data_df_train['tweet'])                                                 #fitting on training set\n",
        "  return tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXjUU0T_7e1s"
      },
      "outputs": [],
      "source": [
        "#feature extraction using Bag-of-words\n",
        "def bow_feature_eng():\n",
        "  bow=CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1,3), analyzer='word', max_features=5000)  #creating unigram and bigrams      \n",
        "  bow.fit(data_df_train['tweet'])                                               #fitting on training set\n",
        "  return bow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BGpH_Bh68B3"
      },
      "outputs": [],
      "source": [
        "#feature extraction using glove embedding \n",
        "def embedding_matrix_creation(word_index):\n",
        " \n",
        "  num_words_feature=len(word_index)+1                                            #word_index vocabulary length\n",
        "\n",
        "  embedding_dict={}\n",
        "  with open('/content/semeval-tweets/glove.6B.100d.txt','r') as f:               #loading glove embedding. *Please change according to data directory*  \n",
        "      for line in f:\n",
        "          values=line.split()                                                    \n",
        "          word=values[0]                                                  \n",
        "          vectors=np.asarray(values[1:],'float32')\n",
        "          embedding_dict[word]=vectors                                           #dict of words and word vectors\n",
        "  f.close()\n",
        "  \n",
        "  nb_words = min(max_features+1, len(word_index)+1)                                 \n",
        "  embedding_matrix=np.zeros((nb_words,embed_size))                               #building embedding matrix (max_features,embed_size)\n",
        "\n",
        "  for word,i in (word_index.items()):                                            \n",
        "      if i > max_features:\n",
        "          continue\n",
        "\n",
        "      emb_vec=embedding_dict.get(word)\n",
        "      if emb_vec is not None:\n",
        "          embedding_matrix[i]=emb_vec                                            #creating embedding matrix with word and word vectors \n",
        "    \n",
        "  return embedding_matrix\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8NU09hmd9IE"
      },
      "source": [
        "**TRAINING CLASSIFIER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bwITMBjTSge"
      },
      "outputs": [],
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, validation_label):\n",
        "    \n",
        "    classifier.fit(feature_vector_train, label)                         # fit the training dataset on the classifier\n",
        "    predictions = classifier.predict(feature_vector_valid)              # predict the labels on validation dataset\n",
        "    \n",
        "    print(f\"Accuracy of {classifier} on development set: {accuracy_score(predictions, validation_label)}\")\n",
        "    return classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyRTe3K89Tmz"
      },
      "outputs": [],
      "source": [
        "class LSTM_NN(nn.Module):\n",
        "    \n",
        "        def __init__(self):                                               \n",
        "          super(LSTM_NN, self).__init__()                                           #implementing NN using nn.Module  \n",
        "          n_classes = 3                                                             #no. of classes\n",
        "          self.hidden_size = 64                                                     #neurons in hidden layer                                                                          \n",
        "          drp = 0.5                                                                 \n",
        "                                                                       \n",
        "          self.embedding = nn.Embedding(max_features+1, embed_size)                                       #embedding layer with (features,embed_size)\n",
        "          self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))       #initializing weights\n",
        "          self.embedding.weight.requires_grad = False                                                     #freeze gradients                     \n",
        "          self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True)         #LSTM layer set with bidirectional=True\n",
        "          self.linear = nn.Linear(self.hidden_size*4 , 64)                                       #linear layer               \n",
        "          self.relu = nn.ReLU()                                                                           #nonlinear layer with activation=ReLU\n",
        "          self.dropout = nn.Dropout(drp)                                                                  #dropout layer\n",
        "          self.out = nn.Linear(64, n_classes)                                                    #linear output layer \n",
        "        \n",
        "        def forward(self, x):\n",
        "          \n",
        "          embedding_mat = self.embedding(x)                                           \n",
        "          lstm_layer, y = self.lstm(embedding_mat)                                        \n",
        "          avg_pooled = torch.mean(lstm_layer, 1)                                           #average pooling of output from LSTM layer\n",
        "          max_pooled, y = torch.max(lstm_layer, 1)                                         #maximum pooling\n",
        "          pooled_layer = torch.cat((avg_pooled, max_pooled), 1)                                 \n",
        "          pooled_layer = self.relu(self.linear(pooled_layer))                              #running linear computations of max pooling and avg pooling and then ReLU activation\n",
        "          pooled_layer = self.dropout(pooled_layer)                                        #dropout layer\n",
        "          out = self.out(pooled_layer)                                                     #output layer     \n",
        "          return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OC6QF5oqk8k"
      },
      "source": [
        "**PREDICTIONS AND EVALUATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztzkvPOgBv27"
      },
      "outputs": [],
      "source": [
        "def predict(classifier,feature_vector_test, test_label):\n",
        "\n",
        "  predictions=classifier.predict(feature_vector_test)                                #predictions on testset\n",
        "  dict_pred=create_dict_from_predictions(predictions,test_label)                     #returning dict of predictions    \n",
        "  return(dict_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGZsZAoh41Gi"
      },
      "outputs": [],
      "source": [
        "def create_dict_from_predictions(predictions,test_label):  \n",
        "  pred=pd.DataFrame({'Id':(test_label.index),'Sentiment':predictions})                 \n",
        "  pred['Sentiment']=pred['Sentiment'].map({0:'negative',1:'neutral',2:'positive'})   #mapping predictions back to original class labels\n",
        "  pred=pred.applymap(str)\n",
        "  return(dict(pred.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c572l3JnmQxm"
      },
      "outputs": [],
      "source": [
        "#Evaluation code for the test sets\n",
        "def read_test(testset):\n",
        "    '''\n",
        "    readin the testset and return a dictionary\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    '''\n",
        "    id_gts = {}\n",
        "    with open(testset, 'r', encoding='utf8') as fh:\n",
        "        for line in fh:\n",
        "            fields = line.split('\\t')\n",
        "            tweetid = fields[0]\n",
        "            gt = fields[1]\n",
        "\n",
        "            id_gts[tweetid] = gt\n",
        "\n",
        "    return id_gts\n",
        "\n",
        "\n",
        "def confusion(id_preds, testset, classifier):\n",
        "    '''\n",
        "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
        "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    :classifier: str, the name of the classifier\n",
        "    '''\n",
        "    id_gts = read_test(testset)\n",
        "\n",
        "    gts = []\n",
        "    for m, c1 in id_gts.items():\n",
        "        if c1 not in gts:\n",
        "            gts.append(c1)\n",
        "\n",
        "    gts = ['positive', 'negative', 'neutral']\n",
        "\n",
        "    conf = {}\n",
        "    for c1 in gts:\n",
        "        conf[c1] = {}\n",
        "        for c2 in gts:\n",
        "            conf[c1][c2] = 0\n",
        "\n",
        "    for tweetid, gt in id_gts.items():\n",
        "        if tweetid in id_preds:\n",
        "            pred = id_preds[tweetid]\n",
        "        else:\n",
        "            pred = 'neutral'\n",
        "        conf[pred][gt] += 1\n",
        "\n",
        "    print(''.ljust(12) + '  '.join(gts))\n",
        "\n",
        "    for c1 in gts:\n",
        "        print(c1.ljust(12), end='')\n",
        "        for c2 in gts:\n",
        "            if sum(conf[c1].values()) > 0:\n",
        "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
        "            else:\n",
        "                print('0.000     ', end='')\n",
        "        print('')\n",
        "\n",
        "    print('')\n",
        "\n",
        "\n",
        "def evaluate(id_preds, testset, classifier):\n",
        "    '''\n",
        "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
        "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    :classifier: str, the name of the classifier\n",
        "    '''\n",
        "    id_gts = read_test(testset)\n",
        "\n",
        "    acc_by_class = {}\n",
        "    for gt in ['positive', 'negative', 'neutral']:\n",
        "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
        "\n",
        "    catf1s = {}\n",
        "\n",
        "    ok = 0\n",
        "    for tweetid, gt in id_gts.items():\n",
        "        if tweetid in id_preds:\n",
        "            pred = id_preds[tweetid]\n",
        "        else:\n",
        "            pred = 'neutral'\n",
        "\n",
        "        if gt == pred:\n",
        "            ok += 1\n",
        "            acc_by_class[gt]['tp'] += 1\n",
        "        else:\n",
        "            acc_by_class[gt]['fn'] += 1\n",
        "            acc_by_class[pred]['fp'] += 1\n",
        "\n",
        "    catcount = 0\n",
        "    itemcount = 0\n",
        "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "\n",
        "    microtp = 0\n",
        "    microfp = 0\n",
        "    microtn = 0\n",
        "    microfn = 0\n",
        "    \n",
        "    for cat, acc in acc_by_class.items():\n",
        "        catcount += 1\n",
        "\n",
        "        microtp += acc['tp']\n",
        "        microfp += acc['fp']\n",
        "        microtn += acc['tn']\n",
        "        microfn += acc['fn']\n",
        "     \n",
        "        p = 0\n",
        "        if (acc['tp'] + acc['fp']) > 0:\n",
        "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
        "\n",
        "        r = 0\n",
        "        if (acc['tp'] + acc['fn']) > 0:\n",
        "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
        "\n",
        "        f1 = 0\n",
        "        if (p + r) > 0:\n",
        "            f1 = 2 * p * r / (p + r)\n",
        "\n",
        "        catf1s[cat] = f1\n",
        "\n",
        "        n = acc['tp'] + acc['fn']\n",
        "\n",
        "        macro['p'] += p\n",
        "        macro['r'] += r\n",
        "        macro['f1'] += f1\n",
        "\n",
        "        if cat in ['positive', 'negative']:\n",
        "            semevalmacro['p'] += p\n",
        "            semevalmacro['r'] += r\n",
        "            semevalmacro['f1'] += f1\n",
        "\n",
        "        itemcount += n\n",
        "\n",
        "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
        "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
        "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
        "\n",
        "\n",
        "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
        "\n",
        "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7agwm41ymQxt"
      },
      "source": [
        "**BUILD sentiment classifiers**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro2MSdBAmQxv",
        "outputId": "1a39784c-3a63-4985-9691-5c0e1f1e4b11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training NaiveBayes with tfidf features\n",
            "Accuracy of MultinomialNB() on development set: 0.629\n",
            "            positive  negative  neutral\n",
            "positive    0.692     0.071     0.237     \n",
            "negative    0.137     0.735     0.127     \n",
            "neutral     0.292     0.175     0.532     \n",
            "\n",
            "semeval-tweets/twitter-test1.txt (tfidf-NaiveBayes): 0.415\n",
            "            positive  negative  neutral\n",
            "positive    0.742     0.049     0.209     \n",
            "negative    0.062     0.875     0.062     \n",
            "neutral     0.386     0.131     0.484     \n",
            "\n",
            "semeval-tweets/twitter-test2.txt (tfidf-NaiveBayes): 0.448\n",
            "            positive  negative  neutral\n",
            "positive    0.713     0.077     0.210     \n",
            "negative    0.193     0.639     0.169     \n",
            "neutral     0.330     0.159     0.511     \n",
            "\n",
            "semeval-tweets/twitter-test3.txt (tfidf-NaiveBayes): 0.401\n",
            "\n",
            "Training NaiveBayes with bow features\n",
            "Accuracy of MultinomialNB() on development set: 0.6255\n",
            "            positive  negative  neutral\n",
            "positive    0.648     0.066     0.286     \n",
            "negative    0.249     0.556     0.196     \n",
            "neutral     0.279     0.143     0.578     \n",
            "\n",
            "semeval-tweets/twitter-test1.txt (bow-NaiveBayes): 0.534\n",
            "            positive  negative  neutral\n",
            "positive    0.714     0.050     0.237     \n",
            "negative    0.150     0.606     0.244     \n",
            "neutral     0.376     0.099     0.524     \n",
            "\n",
            "semeval-tweets/twitter-test2.txt (bow-NaiveBayes): 0.581\n",
            "            positive  negative  neutral\n",
            "positive    0.684     0.069     0.246     \n",
            "negative    0.265     0.458     0.277     \n",
            "neutral     0.316     0.125     0.559     \n",
            "\n",
            "semeval-tweets/twitter-test3.txt (bow-NaiveBayes): 0.519\n",
            "\n",
            "Training RandomForest with tfidf features\n",
            "Accuracy of RandomForestClassifier() on development set: 0.6405\n",
            "            positive  negative  neutral\n",
            "positive    0.785     0.055     0.160     \n",
            "negative    0.122     0.796     0.082     \n",
            "neutral     0.291     0.171     0.538     \n",
            "\n",
            "semeval-tweets/twitter-test1.txt (tfidf-RandomForest): 0.423\n",
            "            positive  negative  neutral\n",
            "positive    0.837     0.045     0.118     \n",
            "negative    0.065     0.839     0.097     \n",
            "neutral     0.384     0.123     0.493     \n",
            "\n",
            "semeval-tweets/twitter-test2.txt (tfidf-RandomForest): 0.436\n",
            "            positive  negative  neutral\n",
            "positive    0.755     0.078     0.167     \n",
            "negative    0.152     0.697     0.152     \n",
            "neutral     0.336     0.157     0.507     \n",
            "\n",
            "semeval-tweets/twitter-test3.txt (tfidf-RandomForest): 0.381\n",
            "\n",
            "Training RandomForest with bow features\n",
            "Accuracy of RandomForestClassifier(n_estimators=50) on development set: 0.6295\n",
            "            positive  negative  neutral\n",
            "positive    0.780     0.059     0.161     \n",
            "negative    0.140     0.729     0.131     \n",
            "neutral     0.295     0.170     0.536     \n",
            "\n",
            "semeval-tweets/twitter-test1.txt (bow-RandomForest): 0.417\n",
            "            positive  negative  neutral\n",
            "positive    0.824     0.047     0.129     \n",
            "negative    0.053     0.868     0.079     \n",
            "neutral     0.386     0.118     0.496     \n",
            "\n",
            "semeval-tweets/twitter-test2.txt (bow-RandomForest): 0.462\n",
            "            positive  negative  neutral\n",
            "positive    0.764     0.069     0.167     \n",
            "negative    0.188     0.662     0.150     \n",
            "neutral     0.335     0.157     0.508     \n",
            "\n",
            "semeval-tweets/twitter-test3.txt (bow-RandomForest): 0.393\n",
            "\n",
            "Training LogisticRegression with tfidf features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of LogisticRegression(random_state=42) on development set: 0.668\n",
            "            positive  negative  neutral\n",
            "positive    0.758     0.058     0.184     \n",
            "negative    0.152     0.730     0.118     \n",
            "neutral     0.269     0.155     0.576     \n",
            "\n",
            "semeval-tweets/twitter-test1.txt (tfidf-LogisticRegression): 0.522\n",
            "            positive  negative  neutral\n",
            "positive    0.804     0.046     0.150     \n",
            "negative    0.114     0.771     0.114     \n",
            "neutral     0.371     0.108     0.521     \n",
            "\n",
            "semeval-tweets/twitter-test2.txt (tfidf-LogisticRegression): 0.539\n",
            "            positive  negative  neutral\n",
            "positive    0.769     0.057     0.173     \n",
            "negative    0.220     0.589     0.190     \n",
            "neutral     0.314     0.146     0.540     \n",
            "\n",
            "semeval-tweets/twitter-test3.txt (tfidf-LogisticRegression): 0.487\n",
            "\n",
            "Training LogisticRegression with bow features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of LogisticRegression(random_state=42) on development set: 0.6545\n",
            "            positive  negative  neutral\n",
            "positive    0.738     0.057     0.205     \n",
            "negative    0.162     0.653     0.185     \n",
            "neutral     0.282     0.142     0.577     \n",
            "\n",
            "semeval-tweets/twitter-test1.txt (bow-LogisticRegression): 0.546\n",
            "            positive  negative  neutral\n",
            "positive    0.794     0.035     0.171     \n",
            "negative    0.167     0.642     0.192     \n",
            "neutral     0.386     0.099     0.515     \n",
            "\n",
            "semeval-tweets/twitter-test2.txt (bow-LogisticRegression): 0.574\n",
            "            positive  negative  neutral\n",
            "positive    0.741     0.058     0.201     \n",
            "negative    0.211     0.512     0.277     \n",
            "neutral     0.336     0.132     0.532     \n",
            "\n",
            "semeval-tweets/twitter-test3.txt (bow-LogisticRegression): 0.499\n",
            "\n",
            "Training LSTM with glove features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:145: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6 \t loss=995.9552 \t val_loss=898.0262  \t val_acc=0.5525  \t time=71.00s\n",
            "Epoch 2/6 \t loss=882.6932 \t val_loss=816.5131  \t val_acc=0.6175  \t time=70.49s\n",
            "Epoch 3/6 \t loss=834.8332 \t val_loss=789.1059  \t val_acc=0.6380  \t time=70.08s\n",
            "Epoch 4/6 \t loss=805.8140 \t val_loss=780.4654  \t val_acc=0.6335  \t time=70.07s\n",
            "Epoch 5/6 \t loss=782.4602 \t val_loss=764.9137  \t val_acc=0.6555  \t time=69.29s\n",
            "Epoch 6/6 \t loss=761.1100 \t val_loss=760.6739  \t val_acc=0.6465  \t time=70.41s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:188: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            positive  negative  neutral\n",
            "positive    0.714     0.050     0.235     \n",
            "negative    0.184     0.693     0.123     \n",
            "neutral     0.291     0.130     0.579     \n",
            "\n",
            "semeval-tweets/twitter-test1.txt (glove-LSTM): 0.569\n",
            "            positive  negative  neutral\n",
            "positive    0.752     0.043     0.205     \n",
            "negative    0.186     0.600     0.214     \n",
            "neutral     0.410     0.089     0.502     \n",
            "\n",
            "semeval-tweets/twitter-test2.txt (glove-LSTM): 0.569\n",
            "            positive  negative  neutral\n",
            "positive    0.710     0.054     0.237     \n",
            "negative    0.246     0.553     0.201     \n",
            "neutral     0.343     0.119     0.538     \n",
            "\n",
            "semeval-tweets/twitter-test3.txt (glove-LSTM): 0.521\n"
          ]
        }
      ],
      "source": [
        "# Build traditional sentiment classifiers. An example classifier name 'svm' is given\n",
        "# in the code below. You should replace the other two classifier names\n",
        "# with your own choices. For features used for classifier training, \n",
        "# the 'bow' feature is given in the code. But you could also explore the \n",
        "# use of other features.\n",
        "#for classifier in ['svm', '<classifier-2-name>', '<classifier-3-name>']:\n",
        "\n",
        "Y_test={}                                                                         #dictionary for testsets                                                            \n",
        "tf_feat={}                                                                        #dictionary for testset tfidf features\n",
        "bow_feat={}                                                                       #dictionary for testset bow features\n",
        "neural_network=False\n",
        "\n",
        "\n",
        "for classifier in ['NaiveBayes', 'RandomForest', 'LogisticRegression', 'LSTM']:\n",
        "    for features in ['tfidf', 'bow', 'glove']:\n",
        "        # Skeleton: Creation and training of the classifiers\n",
        "        if classifier == 'NaiveBayes' and features =='tfidf':\n",
        "\n",
        "            print('Training ' + classifier+ ' with ' + features + ' features')              \n",
        "\n",
        "            data_df_train['tweet']=data_df_train['tweet'].apply(lambda x: preprocess_text(x))         #preprocessing training dataset\n",
        "            data_df_dev['tweet']=data_df_dev['tweet'].apply(lambda x: preprocess_text(x))             #preprocessing dev dataset\n",
        "            \n",
        "            data_df_train['tweet']=data_df_train['tweet'].apply(lambda x: deEmojify(x))\n",
        "            data_df_dev['tweet']=data_df_dev['tweet'].apply(lambda x: deEmojify(x))\n",
        "\n",
        "            data_df_train['tweet']=data_df_train['tweet'].apply(lambda x: stem_words(x))\n",
        "            data_df_dev['tweet']=data_df_dev['tweet'].apply(lambda x: stem_words(x))\n",
        "\n",
        "            Y_target_train= data_df_train['sentiment']                                                #creating target variable for training set\n",
        "            Y_target_dev= data_df_dev['sentiment']                                                    #creating target variable for dev set\n",
        "               \n",
        "            Y_target_train=Y_target_train.map({'negative':0,'neutral':1,'positive':2})                #map class labels to 0,1,2 values \n",
        "            Y_target_dev=Y_target_dev.map({'negative':0,'neutral':1,'positive':2})\n",
        "            \n",
        "            tf=tf_feature_eng()                                                                       #feature extraction using tf-idf\n",
        "            tf_features_train= tf.transform(data_df_train['tweet'])                                   #getting features for train set                                             #tf_features is a sparse matrix [no.of doc*no.of features]\n",
        "            tf_features_dev=tf.transform(data_df_dev['tweet'])                                        #getting features for dev set \n",
        "            \n",
        "            k=0\n",
        "            for i in df_test:\n",
        "              k=k+1\n",
        "              df_test[i]['tweet']= df_test[i]['tweet'].apply(lambda x: preprocess_text(x))            #preprocessing test dataset\n",
        "              df_test[i]['tweet']=df_test[i]['tweet'].apply(lambda x: deEmojify(x))\n",
        "              df_test[i]['tweet']=df_test[i]['tweet'].apply(lambda x: stem_words(x))\n",
        "              Y_test[\"Y_target_test\"+str(k)]= df_test[i]['sentiment']                                 #creating target variable for test dataset\n",
        "              Y_test[\"Y_target_test\"+str(k)]=Y_test[\"Y_target_test\"+str(k)].map({'negative':0,'neutral':1,'positive':2})\n",
        "              tf_feat[\"tf_features_test\"+str(k)]=tf.transform(df_test[i]['tweet'])  \n",
        "            \n",
        "            model_trained=train_model(MultinomialNB(), tf_features_train,Y_target_train,tf_features_dev,Y_target_dev)    \n",
        "    \n",
        "        elif classifier == 'RandomForest' and features=='tfidf':\n",
        "            \n",
        "            print('\\nTraining ' + classifier+ ' with ' + features + ' features')\n",
        "            model_trained=train_model(RandomForestClassifier(n_estimators=100),tf_features_train,Y_target_train,tf_features_dev,Y_target_dev)\n",
        "\n",
        "        elif classifier == 'LogisticRegression' and features == 'tfidf':\n",
        "            \n",
        "            print('\\nTraining ' + classifier+ ' with ' + features + ' features')\n",
        "            model_trained=train_model(LogisticRegression(random_state=42),tf_features_train,Y_target_train,tf_features_dev,Y_target_dev)\n",
        "\n",
        "        elif classifier == 'NaiveBayes' and features =='bow':\n",
        "\n",
        "            print('\\nTraining ' + classifier+ ' with ' + features + ' features')\n",
        "            bow=bow_feature_eng()                                                       #feature extraction using bag-of-words\n",
        "            bow_features_train= bow.transform(data_df_train['tweet'])                                                                                        #tf_features is a sparse matrix [no.of doc*no.of features]\n",
        "            bow_features_dev=bow.transform(data_df_dev['tweet'])           \n",
        "            \n",
        "            k=0\n",
        "            for i in df_test:\n",
        "              k=k+1\n",
        "              bow_feat[\"bow_features_test\"+str(k)]=bow.transform(df_test[i]['tweet'])  \n",
        "\n",
        "            model_trained=train_model(MultinomialNB(),bow_features_train,Y_target_train,bow_features_dev,Y_target_dev)\n",
        "        \n",
        "        elif classifier == 'RandomForest' and features =='bow':\n",
        "\n",
        "            print('\\nTraining ' + classifier+ ' with ' + features + ' features')\n",
        "            model_trained=train_model(RandomForestClassifier(n_estimators=50),bow_features_train,Y_target_train,bow_features_dev,Y_target_dev)\n",
        "        \n",
        "        elif classifier == 'LogisticRegression' and features =='bow':\n",
        "\n",
        "            print('\\nTraining ' + classifier+ ' with ' + features + ' features')\n",
        "            model_trained=train_model(LogisticRegression(random_state=42),bow_features_train,Y_target_train,bow_features_dev,Y_target_dev)\n",
        "\n",
        "        elif classifier == 'LSTM' and features=='glove':\n",
        "\n",
        "            print('\\nTraining ' + classifier+ ' with ' + features + ' features')\n",
        "\n",
        "            neural_network=True\n",
        "            tokenizer = Tokenizer(num_words=5000)                                 #Tokenize sentences to generate vocabulary of words\n",
        "            tokenizer.fit_on_texts(list(data_df_train['tweet']))                  #fit tokenizer on train data    \n",
        "            train_X = tokenizer.texts_to_sequences(data_df_train['tweet'])        #convert train text to integer sequences\n",
        "            dev_X = tokenizer.texts_to_sequences(data_df_dev['tweet'])            #convert dev text to integer sequences\n",
        "            train_X = pad_sequences(train_X, maxlen=max_len)                      #pad train sequences \n",
        "            dev_X = pad_sequences(dev_X, maxlen=max_len)\n",
        "            train_y=Y_target_train.values                                         #get target train class label values\n",
        "            \n",
        "            dev_y=Y_target_dev.values                                             \n",
        "\n",
        "            embedding_matrix=embedding_matrix_creation(tokenizer.word_index)      #create embedding matrix from glove\n",
        "\n",
        "            n_epochs = 6    \n",
        "            model = LSTM_NN()  \n",
        "            loss_function = nn.CrossEntropyLoss(reduction='sum')                  #setting loss fn\n",
        "            optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.003)         #setting Adam optimizer\n",
        "            \n",
        "            x_train = torch.tensor(train_X, dtype=torch.long)                     #tensor for tweets of training data\n",
        "            y_train = torch.tensor(train_y, dtype=torch.long)                     #tensor for sentiment of training data        \n",
        "            \n",
        "            x_dev = torch.tensor(dev_X, dtype=torch.long)                         #tensor for tweets of dev data\n",
        "            y_dev = torch.tensor(dev_y, dtype=torch.long)                         #tensor for sentiment of dev data   \n",
        "\n",
        "            train = torch.utils.data.TensorDataset(x_train, y_train)               # Create torch datasets    \n",
        "            valid = torch.utils.data.TensorDataset(x_dev, y_dev)\n",
        "           \n",
        "            train_loader = torch.utils.data.DataLoader(train, batch_size=1024, shuffle=True)       # Create data loaders for training data\n",
        "            valid_loader = torch.utils.data.DataLoader(valid, batch_size=1024, shuffle=False)       # Create data loaders for validation data\n",
        "\n",
        "            train_loss = []\n",
        "            valid_loss = []\n",
        "\n",
        "            for epoch in range(n_epochs):\n",
        "                start_time = time.time()\n",
        "               \n",
        "                model.train()                                                                     #training NN model\n",
        "                loss_computed = 0.                   \n",
        "                \n",
        "                for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "                                                                                            \n",
        "                    y_pred = model(x_batch)                                                        # run forward pass to train the model                 \n",
        "                    loss = loss_function(y_pred, y_batch)                                          # Compute loss\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.sum().backward()\n",
        "                    optimizer.step()\n",
        "                    loss_computed += loss.item() / len(train_loader)        \n",
        "\n",
        "                model.eval()                                                                        #evaluate loss for dev set\n",
        "                avg_val_loss = 0.\n",
        "                val_preds = np.zeros((len(x_dev),3))                                 \n",
        "\n",
        "                for i, (x_batch, y_batch) in enumerate(valid_loader):                           \n",
        "                    y_pred = model(x_batch).detach()                                               #run predictions on dev set\n",
        "                    avg_val_loss += loss_function(y_pred, y_batch).item() / len(valid_loader)            #compute loss\n",
        "                    val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred).cpu().numpy()  #running softmax function on predictions\n",
        "\n",
        "                val_accuracy = sum(val_preds.argmax(axis=1)==dev_y)/len(dev_y)                      #accuracy on dev set\n",
        "                train_loss.append(loss_computed)                                                           \n",
        "                valid_loss.append(avg_val_loss)                                                     \n",
        "                elapsed_time = time.time() - start_time \n",
        "                print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(\n",
        "                epoch + 1, n_epochs, loss_computed, avg_val_loss, val_accuracy, elapsed_time))\n",
        "           \n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        #Prediction performance of the classifiers\n",
        "        l=0\n",
        "        for testset in testsets:\n",
        "            \n",
        "            id_preds = {}\n",
        "            l=l+1\n",
        "\n",
        "            if features=='tfidf' and neural_network==False:                      #running predictions on testsets for tf-idf features              \n",
        "              id_preds=predict(model_trained,tf_feat['tf_features_test'+str(l)],Y_test['Y_target_test'+str(l)])\n",
        "\n",
        "              testset_name = testset\n",
        "              testset_path = join('semeval-tweets', testset_name)\n",
        "              confusion(id_preds, testset_path, classifier)                      #generate confusion matrix\n",
        "              evaluate(id_preds, testset_path, features + '-' + classifier)      #get F1 score\n",
        "\n",
        "            elif features=='bow' and neural_network==False:                      #running predictions on testsets for bag-of-words features   \n",
        "              id_preds=predict(model_trained,bow_feat['bow_features_test'+str(l)],Y_test['Y_target_test'+str(l)])\n",
        "\n",
        "              testset_name = testset\n",
        "              testset_path = join('semeval-tweets', testset_name)\n",
        "              confusion(id_preds, testset_path, classifier)\n",
        "              evaluate(id_preds, testset_path, features + '-' + classifier)\n",
        "\n",
        "            else:                                                                #running predictions on testsets for NN\n",
        "              x= df_test['data_df_test'+str(l)]['tweet']                           \n",
        "              x = tokenizer.texts_to_sequences(x)                                #convert text to sequence\n",
        "              x = pad_sequences(x, maxlen=max_len)                               #pad sequence\n",
        "              \n",
        "              x = torch.tensor(x, dtype=torch.long)                           \n",
        "\n",
        "              pred = model(x).detach()                                           #run NN model to get predictions\n",
        "              pred = F.softmax(pred).cpu().numpy()                               #running softmax function on predictions                    \n",
        "\n",
        "              pred = pred.argmax(axis=1)                                          \n",
        "\n",
        "              test_label= df_test['data_df_test'+str(l)]['sentiment']            #get class label  \n",
        "              id_preds=create_dict_from_predictions(pred,test_label)\n",
        "              \n",
        "              testset_name = testset\n",
        "              testset_path = join('semeval-tweets', testset_name)\n",
        "              confusion(id_preds, testset_path, classifier)                      #confusion matrix\n",
        "              evaluate(id_preds, testset_path, features + '-' + classifier)      #F1 score \n",
        "                  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cs918_assignment_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}