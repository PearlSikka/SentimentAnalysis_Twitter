{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs918_assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSKnrUEumQxT"
      },
      "source": [
        "# Assignment Two:  Sentiment Classification\n",
        "\n",
        "For this exercise you will be using the \"SemEval 2017 task 4\" corpus provided on the module website, available through the following link: https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs918/semeval-tweets.tar.bz2 You will focus particularly on Subtask A, i.e. classifying the overall sentiment of a tweet as positive, negative or neutral.\n",
        "\n",
        "You are requested to produce a standalone Python program or Jupyter notebook for coursework submission. The input to your program is the SemEval data downloaded. Note that TAs need to run your program on their own machine by using the original SemEval data. As such, don’t submit a Python program that takes as input some preprocessed files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqtl05vRmQxg"
      },
      "source": [
        "**IMPORT necessary packages**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI58Ej-UmQxi"
      },
      "source": [
        "# Import necessary packages\n",
        "import re\n",
        "import string\n",
        "from os.path import join\n",
        "import numpy as np                                                                #for array manipulations\n",
        "import pandas as pd                                                               #for dataframe manipulations\n",
        "import seaborn as sns                                                             #for plots\n",
        "import nltk                                                                       #to process and manipulate text data\n",
        "from nltk import PorterStemmer\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop=set(stopwords.words('english'))\n",
        "stemmer=PorterStemmer()\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer       #for feature extraction\n",
        "from sklearn import svm                                                           #SVM\n",
        "from sklearn.naive_bayes import MultinomialNB                                     #Naive Bayes\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier                               #RandomForest\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score,f1_score   \n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer                                    #Keras Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences                            \n",
        "\n",
        "import torch                                                                      #pytorch \n",
        "from torch import nn                                                              #pytorch neural network building class\n",
        "from torch.utils import data\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o079KIVruCbo"
      },
      "source": [
        "**LOAD DATASET: Loading training, validation/dev and test datasets.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8C7JBYah4SY"
      },
      "source": [
        "# Define test sets\n",
        "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']      #testsets. *Please add the additional testsets here*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA7_SCUht-XN"
      },
      "source": [
        "datadir= '/content/semeval-tweets/'                                             #load data directory. *Please change according to the location of data*\n",
        "\n",
        "df_test={}                                                        \n",
        "\n",
        "data_df_train=pd.read_csv(datadir +'twitter-training-data.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE, error_bad_lines=False)   #reading training dataset\n",
        "data_df_dev=pd.read_csv(datadir +'twitter-dev-data.txt', sep='\\t', header=None, quoting=csv.QUOTE_NONE, error_bad_lines=False)          #reading dev dataset\n",
        "\n",
        "data_df_train.rename(columns={0:'id',1:'sentiment',2:'tweet'},inplace=True)                                  #to give columns meaningful name\n",
        "data_df_dev.rename(columns={0:'id',1:'sentiment',2:'tweet'},inplace=True)                             \n",
        "\n",
        "data_df_train=data_df_train.set_index('id')                                                                    \n",
        "data_df_dev=data_df_dev.set_index('id')\n",
        "\n",
        "k=0\n",
        "for test in testsets:                                                                                        #reading testsets\n",
        "  k=k+1\n",
        "  df_test['data_df_test'+str(k)]= pd.read_csv(datadir + test, sep='\\t', header=None, quoting=csv.QUOTE_NONE, error_bad_lines=False)\n",
        "  df_test['data_df_test'+str(k)].rename(columns={0:'id',1:'sentiment',2:'tweet'},inplace=True)\n",
        "  df_test['data_df_test'+str(k)]=df_test['data_df_test'+str(k)].set_index('id')\n",
        "\n",
        "data_df_train.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0OAdRSaTy-R"
      },
      "source": [
        "print(f'There are {data_df_train.shape[0]} tweets in training set')\n",
        "print(f'There are {data_df_dev.shape[0]} tweets in dev set')\n",
        "\n",
        "for i in df_test:\n",
        "  print(f'There are {df_test[i].shape[0]} tweets in testset {i}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVpicfB4Ylin"
      },
      "source": [
        "print(f\"Missing sample in training set: {data_df_train.isnull().values.any()}\")\n",
        "print(f\"Missing sample in development set: {data_df_dev.isnull().values.any()}\")\n",
        "\n",
        "for i in df_test:\n",
        "  print(f\"Missing sample in test set: {df_test[i].isnull().values.any()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W8euQbiSHn7"
      },
      "source": [
        "**CLASS DISTRIBUTION**: Before starting with analysis, it's important to see the distribution of values in classes in training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpbrABDCSLEd"
      },
      "source": [
        "sns.countplot(x='sentiment', data=data_df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIRcv11lyz7k"
      },
      "source": [
        "**PREPROCESS TWEETS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJnaLyJxywjU"
      },
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    text = re.sub(r\"he's\", \"he is\", text)                                        #removing contractions\n",
        "    text = re.sub(r\"there's\", \"there is\", text)\n",
        "    text = re.sub(r\"We're\", \"We are\", text)\n",
        "    text = re.sub(r\"That's\", \"That is\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"they're\", \"they are\", text)\n",
        "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
        "    text = re.sub(r\"Can't\", \"Cannot\", text)\n",
        "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
        "    text = re.sub(r\"don\\x89Ûªt\", \"do not\", text)\n",
        "    text = re.sub(r\"aren't\", \"are not\", text)\n",
        "    text = re.sub(r\"isn't\", \"is not\", text)\n",
        "    text = re.sub(r\"What's\", \"What is\", text)\n",
        "    text = re.sub(r\"haven't\", \"have not\", text)\n",
        "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
        "    text = re.sub(r\"There's\", \"There is\", text)\n",
        "    text = re.sub(r\"He's\", \"He is\", text)\n",
        "    text = re.sub(r\"It's\", \"It is\", text)\n",
        "    text = re.sub(r\"You're\", \"You are\", text)\n",
        "    text = re.sub(r\"I'M\", \"I am\", text)\n",
        "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
        "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
        "    text = re.sub(r\"i'm\", \"I am\", text)\n",
        "    text = re.sub(r\"I\\x89Ûªm\", \"I am\", text)\n",
        "    text = re.sub(r\"I'm\", \"I am\", text)\n",
        "    text = re.sub(r\"Isn't\", \"is not\", text)\n",
        "    text = re.sub(r\"Here's\", \"Here is\", text)\n",
        "    text = re.sub(r\"you've\", \"you have\", text)\n",
        "    text = re.sub(r\"you\\x89Ûªve\", \"you have\", text)\n",
        "    text = re.sub(r\"we're\", \"we are\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
        "    text = re.sub(r\"we've\", \"we have\", text)\n",
        "    text = re.sub(r\"it\\x89Ûªs\", \"it is\", text)\n",
        "    text = re.sub(r\"doesn\\x89Ûªt\", \"does not\", text)\n",
        "    text = re.sub(r\"It\\x89Ûªs\", \"It is\", text)\n",
        "    text = re.sub(r\"Here\\x89Ûªs\", \"Here is\", text)\n",
        "    text = re.sub(r\"who's\", \"who is\", text)\n",
        "    text = re.sub(r\"I\\x89Ûªve\", \"I have\", text)\n",
        "    text = re.sub(r\"y'all\", \"you all\", text)\n",
        "    text = re.sub(r\"can\\x89Ûªt\", \"cannot\", text)\n",
        "    text = re.sub(r\"would've\", \"would have\", text)\n",
        "    text = re.sub(r\"it'll\", \"it will\", text)\n",
        "    text = re.sub(r\"we'll\", \"we will\", text)\n",
        "    text = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", text)\n",
        "    text = re.sub(r\"We've\", \"We have\", text)\n",
        "    text = re.sub(r\"he'll\", \"he will\", text)\n",
        "    text = re.sub(r\"Y'all\", \"You all\", text)\n",
        "    text = re.sub(r\"Weren't\", \"Were not\", text)\n",
        "    text = re.sub(r\"Didn't\", \"Did not\", text)\n",
        "    text = re.sub(r\"they'll\", \"they will\", text)\n",
        "    text = re.sub(r\"they'd\", \"they would\", text)\n",
        "    text = re.sub(r\"DON'T\", \"DO NOT\", text)\n",
        "    text = re.sub(r\"That\\x89Ûªs\", \"That is\", text)\n",
        "    text = re.sub(r\"they've\", \"they have\", text)\n",
        "    text = re.sub(r\"i'll\", \"I will\", text)\n",
        "    text = re.sub(r\"weren't\", \"were not\", text)\n",
        "    text = re.sub(r\"They're\", \"They are\", text)\n",
        "    text = re.sub(r\"let's\", \"let us\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"don't\", \"do not\", text)\n",
        "    text = re.sub(r\"you're\", \"you are\", text)\n",
        "    text = re.sub(r\"i've\", \"I have\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"i'll\", \"I will\", text)\n",
        "    text = re.sub(r\"doesn't\", \"does not\", text)\n",
        "    text = re.sub(r\"i'd\", \"I would\", text)\n",
        "    text = re.sub(r\"didn't\", \"did not\", text)\n",
        "    text = re.sub(r\"ain't\", \"am not\", text)\n",
        "    text = re.sub(r\"you'll\", \"you will\", text)\n",
        "    text = re.sub(r\"I've\", \"I have\", text)\n",
        "    text = re.sub(r\"Don't\", \"do not\", text)\n",
        "    text = re.sub(r\"I'll\", \"I will\", text)\n",
        "    text = re.sub(r\"I'd\", \"I would\", text)\n",
        "    text = re.sub(r\"Let's\", \"Let us\", text)\n",
        "    text = re.sub(r\"you'd\", \"You would\", text)\n",
        "    text = re.sub(r\"It's\", \"It is\", text)\n",
        "    text = re.sub(r\"Ain't\", \"am not\", text)\n",
        "    text = re.sub(r\"Haven't\", \"Have not\", text)\n",
        "    text = re.sub(r\"Could've\", \"Could have\", text)\n",
        "    text = re.sub(r\"youve\", \"you have\", text)  \n",
        "    text = re.sub(r\"I'm\", \"I am\", text) \n",
        "    text = re.sub(r\"youll\", \"you will\", text)\n",
        "    text = re.sub(r\"wont\", \"will not\", text)\n",
        "    text = re.sub(r\"cant\", \"can not\", text)\n",
        "    text = re.sub(r\"dont\", \"do not\", text)\n",
        "\n",
        "    text=  re.sub(r'^RT[\\s]+', '', text)                                        #substituting for retweet RT \n",
        "    text = str(text).lower()                                                    #lowercase characters \n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)                            #substituting URLs \n",
        "    text = re.sub('\\[.*?\\]', '', text)                                          #substituting for []\n",
        "    text = re.sub('\\n', '', text)                                               #substituting new line  \n",
        "    text = re.sub('\\w*\\d\\w*', '', text)                                         #substituting digits  \n",
        "    text = re.sub('<.:*?>+', '', text)                                          #substituting for <>\n",
        "    text=  re.sub(r'#', '', text)                                               #substituting for hashtag. Not removing the entire hashtag word\n",
        "    text=  re.sub('@\\w*','',text)                                               #substituting for @\n",
        "    text = re.sub(r'\\s+', ' ', text)                                            #substituting multiple spaces with one space\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)            #removing punctuations\n",
        "                  \n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5pEhFJ-9CYb"
      },
      "source": [
        "def deEmojify(text):                                                                  \n",
        "  re_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "  \n",
        "  return re_pattern.sub(r'',text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ndD4oQORiVY"
      },
      "source": [
        "def stem_words(text):                      \n",
        "    split_word=text.split()                           \n",
        "    cleaned_word=\" \".join([stemmer.stem(i) for i in split_word])                   #to stem the words\n",
        "    cleaned_word= re.sub(r'\\b\\w{1}\\b','',cleaned_word)                              #to remove words of length 1 \n",
        "    return cleaned_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU_w5BZ6rABq"
      },
      "source": [
        "embed_size = 100                                                                  # how big is each word vector\n",
        "max_features = 5000                                                               # how many unique words to use (i.e num rows in embedding vector)\n",
        "max_len = 100                                                                     # max number of words in a question to use\n",
        "batch_size = 1024                                                                 # how many samples to process at once"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVRxaQ3_bRl3"
      },
      "source": [
        "**FEATURE SELECTION AND ENGINEERING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBxnjt-8Y5c2"
      },
      "source": [
        "#feature extraction using TF-IDF \n",
        "def tf_feature_eng():\n",
        "  tf=TfidfVectorizer(min_df=1, max_df=0.9, ngram_range=(1,3), analyzer='word', max_features=5000)       #creating unigram and bigrams      \n",
        "  tf.fit(data_df_train['tweet'])                                                 #fitting on training set\n",
        "  return tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXjUU0T_7e1s"
      },
      "source": [
        "#feature extraction using Bag-of-words\n",
        "def bow_feature_eng():\n",
        "  bow=CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1,3), analyzer='word', max_features=5000)  #creating unigram and bigrams      \n",
        "  bow.fit(data_df_train['tweet'])                                               #fitting on training set\n",
        "  return bow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BGpH_Bh68B3"
      },
      "source": [
        "#feature extraction using glove embedding \n",
        "def embedding_matrix_creation(word_index):\n",
        " \n",
        "  num_words_feature=len(word_index)+1                                            #word_index vocabulary length\n",
        "\n",
        "  embedding_dict={}\n",
        "  with open('/content/semeval-tweets/glove.6B.100d.txt','r') as f:               #loading glove embedding. *Please change according to data directory*  \n",
        "      for line in f:\n",
        "          values=line.split()                                                    \n",
        "          word=values[0]                                                  \n",
        "          vectors=np.asarray(values[1:],'float32')\n",
        "          embedding_dict[word]=vectors                                           #dict of words and word vectors\n",
        "  f.close()\n",
        "  \n",
        "  nb_words = min(max_features+1, len(word_index)+1)                                 \n",
        "  embedding_matrix=np.zeros((nb_words,embed_size))                               #building embedding matrix (max_features,embed_size)\n",
        "\n",
        "  for word,i in (word_index.items()):                                            \n",
        "      if i > max_features:\n",
        "          continue\n",
        "\n",
        "      emb_vec=embedding_dict.get(word)\n",
        "      if emb_vec is not None:\n",
        "          embedding_matrix[i]=emb_vec                                            #creating embedding matrix with word and word vectors \n",
        "    \n",
        "  return embedding_matrix\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8NU09hmd9IE"
      },
      "source": [
        "**TRAINING CLASSIFIER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bwITMBjTSge"
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, validation_label):\n",
        "    \n",
        "    classifier.fit(feature_vector_train, label)                         # fit the training dataset on the classifier\n",
        "    predictions = classifier.predict(feature_vector_valid)              # predict the labels on validation dataset\n",
        "    \n",
        "    print(f\"Accuracy of {classifier} on development set: {accuracy_score(predictions, validation_label)}\")\n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyRTe3K89Tmz"
      },
      "source": [
        "class LSTM_NN(nn.Module):\n",
        "    \n",
        "        def __init__(self):                                               \n",
        "          super(LSTM_NN, self).__init__()                                           #implementing NN using nn.Module  \n",
        "          n_classes = 3                                                             #no. of classes\n",
        "          self.hidden_size = 64                                                     #neurons in hidden layer                                                                          \n",
        "          drp = 0.5                                                                 \n",
        "                                                                       \n",
        "          self.embedding = nn.Embedding(max_features+1, embed_size)                                       #embedding layer with (features,embed_size)\n",
        "          self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))       #initializing weights\n",
        "          self.embedding.weight.requires_grad = False                                                     #freeze gradients                     \n",
        "          self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True)         #LSTM layer set with bidirectional=True\n",
        "          self.linear = nn.Linear(self.hidden_size*4 , 64)                                       #linear layer               \n",
        "          self.relu = nn.ReLU()                                                                           #nonlinear layer with activation=ReLU\n",
        "          self.dropout = nn.Dropout(drp)                                                                  #dropout layer\n",
        "          self.out = nn.Linear(64, n_classes)                                                    #linear output layer \n",
        "        \n",
        "        def forward(self, x):\n",
        "          \n",
        "          embedding_mat = self.embedding(x)                                           \n",
        "          lstm_layer, y = self.lstm(embedding_mat)                                        \n",
        "          avg_pooled = torch.mean(lstm_layer, 1)                                           #average pooling of output from LSTM layer\n",
        "          max_pooled, y = torch.max(lstm_layer, 1)                                         #maximum pooling\n",
        "          pooled_layer = torch.cat((avg_pooled, max_pooled), 1)                                 \n",
        "          pooled_layer = self.relu(self.linear(pooled_layer))                              #running linear computations of max pooling and avg pooling and then ReLU activation\n",
        "          pooled_layer = self.dropout(pooled_layer)                                        #dropout layer\n",
        "          out = self.out(pooled_layer)                                                     #output layer     \n",
        "          return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREDICTIONS AND EVALUATION**"
      ],
      "metadata": {
        "id": "5OC6QF5oqk8k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztzkvPOgBv27"
      },
      "source": [
        "def predict(classifier,feature_vector_test, test_label):\n",
        "\n",
        "  predictions=classifier.predict(feature_vector_test)                                #predictions on testset\n",
        "  dict_pred=create_dict_from_predictions(predictions,test_label)                     #returning dict of predictions    \n",
        "  return(dict_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGZsZAoh41Gi"
      },
      "source": [
        "def create_dict_from_predictions(predictions,test_label):  \n",
        "  pred=pd.DataFrame({'Id':(test_label.index),'Sentiment':predictions})                 \n",
        "  pred['Sentiment']=pred['Sentiment'].map({0:'negative',1:'neutral',2:'positive'})   #mapping predictions back to original class labels\n",
        "  pred=pred.applymap(str)\n",
        "  return(dict(pred.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c572l3JnmQxm"
      },
      "source": [
        "#Evaluation code for the test sets\n",
        "def read_test(testset):\n",
        "    '''\n",
        "    readin the testset and return a dictionary\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    '''\n",
        "    id_gts = {}\n",
        "    with open(testset, 'r', encoding='utf8') as fh:\n",
        "        for line in fh:\n",
        "            fields = line.split('\\t')\n",
        "            tweetid = fields[0]\n",
        "            gt = fields[1]\n",
        "\n",
        "            id_gts[tweetid] = gt\n",
        "\n",
        "    return id_gts\n",
        "\n",
        "\n",
        "def confusion(id_preds, testset, classifier):\n",
        "    '''\n",
        "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
        "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    :classifier: str, the name of the classifier\n",
        "    '''\n",
        "    id_gts = read_test(testset)\n",
        "\n",
        "    gts = []\n",
        "    for m, c1 in id_gts.items():\n",
        "        if c1 not in gts:\n",
        "            gts.append(c1)\n",
        "\n",
        "    gts = ['positive', 'negative', 'neutral']\n",
        "\n",
        "    conf = {}\n",
        "    for c1 in gts:\n",
        "        conf[c1] = {}\n",
        "        for c2 in gts:\n",
        "            conf[c1][c2] = 0\n",
        "\n",
        "    for tweetid, gt in id_gts.items():\n",
        "        if tweetid in id_preds:\n",
        "            pred = id_preds[tweetid]\n",
        "        else:\n",
        "            pred = 'neutral'\n",
        "        conf[pred][gt] += 1\n",
        "\n",
        "    print(''.ljust(12) + '  '.join(gts))\n",
        "\n",
        "    for c1 in gts:\n",
        "        print(c1.ljust(12), end='')\n",
        "        for c2 in gts:\n",
        "            if sum(conf[c1].values()) > 0:\n",
        "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
        "            else:\n",
        "                print('0.000     ', end='')\n",
        "        print('')\n",
        "\n",
        "    print('')\n",
        "\n",
        "\n",
        "def evaluate(id_preds, testset, classifier):\n",
        "    '''\n",
        "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
        "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
        "    :param testset: str, the file name of the testset to compare\n",
        "    :classifier: str, the name of the classifier\n",
        "    '''\n",
        "    id_gts = read_test(testset)\n",
        "\n",
        "    acc_by_class = {}\n",
        "    for gt in ['positive', 'negative', 'neutral']:\n",
        "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
        "\n",
        "    catf1s = {}\n",
        "\n",
        "    ok = 0\n",
        "    for tweetid, gt in id_gts.items():\n",
        "        if tweetid in id_preds:\n",
        "            pred = id_preds[tweetid]\n",
        "        else:\n",
        "            pred = 'neutral'\n",
        "\n",
        "        if gt == pred:\n",
        "            ok += 1\n",
        "            acc_by_class[gt]['tp'] += 1\n",
        "        else:\n",
        "            acc_by_class[gt]['fn'] += 1\n",
        "            acc_by_class[pred]['fp'] += 1\n",
        "\n",
        "    catcount = 0\n",
        "    itemcount = 0\n",
        "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
        "\n",
        "    microtp = 0\n",
        "    microfp = 0\n",
        "    microtn = 0\n",
        "    microfn = 0\n",
        "    \n",
        "    for cat, acc in acc_by_class.items():\n",
        "        catcount += 1\n",
        "\n",
        "        microtp += acc['tp']\n",
        "        microfp += acc['fp']\n",
        "        microtn += acc['tn']\n",
        "        microfn += acc['fn']\n",
        "     \n",
        "        p = 0\n",
        "        if (acc['tp'] + acc['fp']) > 0:\n",
        "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
        "\n",
        "        r = 0\n",
        "        if (acc['tp'] + acc['fn']) > 0:\n",
        "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
        "\n",
        "        f1 = 0\n",
        "        if (p + r) > 0:\n",
        "            f1 = 2 * p * r / (p + r)\n",
        "\n",
        "        catf1s[cat] = f1\n",
        "\n",
        "        n = acc['tp'] + acc['fn']\n",
        "\n",
        "        macro['p'] += p\n",
        "        macro['r'] += r\n",
        "        macro['f1'] += f1\n",
        "\n",
        "        if cat in ['positive', 'negative']:\n",
        "            semevalmacro['p'] += p\n",
        "            semevalmacro['r'] += r\n",
        "            semevalmacro['f1'] += f1\n",
        "\n",
        "        itemcount += n\n",
        "\n",
        "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
        "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
        "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
        "\n",
        "\n",
        "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
        "\n",
        "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7agwm41ymQxt"
      },
      "source": [
        "**BUILD sentiment classifiers**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro2MSdBAmQxv"
      },
      "source": [
        "# Build traditional sentiment classifiers. An example classifier name 'svm' is given\n",
        "# in the code below. You should replace the other two classifier names\n",
        "# with your own choices. For features used for classifier training, \n",
        "# the 'bow' feature is given in the code. But you could also explore the \n",
        "# use of other features.\n",
        "#for classifier in ['svm', '<classifier-2-name>', '<classifier-3-name>']:\n",
        "\n",
        "Y_test={}                                                                         #dictionary for testsets                                                            \n",
        "tf_feat={}                                                                        #dictionary for testset tfidf features\n",
        "bow_feat={}                                                                       #dictionary for testset bow features\n",
        "neural_network=False\n",
        "\n",
        "\n",
        "for classifier in ['NaiveBayes', 'RandomForest', 'LogisticRegression', 'LSTM']:\n",
        "    for features in ['tfidf', 'bow', 'glove']:\n",
        "        # Skeleton: Creation and training of the classifiers\n",
        "        if classifier == 'NaiveBayes' and features =='tfidf':\n",
        "\n",
        "            print('Training ' + classifier+ ' with ' + features + ' features')              \n",
        "\n",
        "            data_df_train['tweet']=data_df_train['tweet'].apply(lambda x: preprocess_text(x))         #preprocessing training dataset\n",
        "            data_df_dev['tweet']=data_df_dev['tweet'].apply(lambda x: preprocess_text(x))             #preprocessing dev dataset\n",
        "            \n",
        "            data_df_train['tweet']=data_df_train['tweet'].apply(lambda x: deEmojify(x))\n",
        "            data_df_dev['tweet']=data_df_dev['tweet'].apply(lambda x: deEmojify(x))\n",
        "\n",
        "            data_df_train['tweet']=data_df_train['tweet'].apply(lambda x: stem_words(x))\n",
        "            data_df_dev['tweet']=data_df_dev['tweet'].apply(lambda x: stem_words(x))\n",
        "\n",
        "            Y_target_train= data_df_train['sentiment']                                                #creating target variable for training set\n",
        "            Y_target_dev= data_df_dev['sentiment']                                                    #creating target variable for dev set\n",
        "               \n",
        "            Y_target_train=Y_target_train.map({'negative':0,'neutral':1,'positive':2})                #map class labels to 0,1,2 values \n",
        "            Y_target_dev=Y_target_dev.map({'negative':0,'neutral':1,'positive':2})\n",
        "            \n",
        "            tf=tf_feature_eng()                                                                       #feature extraction using tf-idf\n",
        "            tf_features_train= tf.transform(data_df_train['tweet'])                                   #getting features for train set                                             #tf_features is a sparse matrix [no.of doc*no.of features]\n",
        "            tf_features_dev=tf.transform(data_df_dev['tweet'])                                        #getting features for dev set \n",
        "            \n",
        "            k=0\n",
        "            for i in df_test:\n",
        "              k=k+1\n",
        "              df_test[i]['tweet']= df_test[i]['tweet'].apply(lambda x: preprocess_text(x))            #preprocessing test dataset\n",
        "              df_test[i]['tweet']=df_test[i]['tweet'].apply(lambda x: deEmojify(x))\n",
        "              df_test[i]['tweet']=df_test[i]['tweet'].apply(lambda x: stem_words(x))\n",
        "              Y_test[\"Y_target_test\"+str(k)]= df_test[i]['sentiment']                                 #creating target variable for test dataset\n",
        "              Y_test[\"Y_target_test\"+str(k)]=Y_test[\"Y_target_test\"+str(k)].map({'negative':0,'neutral':1,'positive':2})\n",
        "              tf_feat[\"tf_features_test\"+str(k)]=tf.transform(df_test[i]['tweet'])  \n",
        "            \n",
        "            model_trained=train_model(MultinomialNB(), tf_features_train,Y_target_train,tf_features_dev,Y_target_dev)    \n",
        "    \n",
        "        elif classifier == 'RandomForest' and features=='tfidf':\n",
        "            \n",
        "            print('\\nTraining ' + classifier+ ' with ' + features + ' features')\n",
        "            model_trained=train_model(RandomForestClassifier(n_estimators=100),tf_features_train,Y_target_train,tf_features_dev,Y_target_dev)\n",
        "\n",
        "        elif classifier == 'LogisticRegression' and features == 'tfidf':\n",
        "            \n",
        "            print('\\nTraining ' + classifier+ ' with ' + features + ' features')\n",
        "            model_trained=train_model(LogisticRegression(random_state=42),tf_features_train,Y_target_train,tf_features_dev,Y_target_dev)\n",
        "\n",
        "        elif classifier == 'NaiveBayes' and features =='bow':\n",
        "\n",
        "            print('\\nTraining ' + classifier+ ' with ' + features + ' features')\n",
        "            bow=bow_feature_eng()                                                       #feature extraction using bag-of-words\n",
        "            bow_features_train= bow.transform(data_df_train['tweet'])                                                                                        #tf_features is a sparse matrix [no.of doc*no.of features]\n",
        "            bow_features_dev=bow.transform(data_df_dev['tweet'])           \n",
        "            \n",
        "            k=0\n",
        "            for i in df_test:\n",
        "              k=k+1\n",
        "              bow_feat[\"bow_features_test\"+str(k)]=bow.transform(df_test[i]['tweet'])  \n",
        "\n",
        "            model_trained=train_model(MultinomialNB(),bow_features_train,Y_target_train,bow_features_dev,Y_target_dev)\n",
        "        \n",
        "        elif classifier == 'RandomForest' and features =='bow':\n",
        "\n",
        "            print('\\nTraining ' + classifier+ ' with ' + features + ' features')\n",
        "            model_trained=train_model(RandomForestClassifier(n_estimators=50),bow_features_train,Y_target_train,bow_features_dev,Y_target_dev)\n",
        "        \n",
        "        elif classifier == 'LogisticRegression' and features =='bow':\n",
        "\n",
        "            print('\\nTraining ' + classifier+ ' with ' + features + ' features')\n",
        "            model_trained=train_model(LogisticRegression(random_state=42),bow_features_train,Y_target_train,bow_features_dev,Y_target_dev)\n",
        "\n",
        "        elif classifier == 'LSTM' and features=='glove':\n",
        "\n",
        "            print('\\nTraining ' + classifier+ ' with ' + features + ' features')\n",
        "\n",
        "            neural_network=True\n",
        "            tokenizer = Tokenizer(num_words=5000)                                 #Tokenize sentences to generate vocabulary of words\n",
        "            tokenizer.fit_on_texts(list(data_df_train['tweet']))                  #fit tokenizer on train data    \n",
        "            train_X = tokenizer.texts_to_sequences(data_df_train['tweet'])        #convert train text to integer sequences\n",
        "            dev_X = tokenizer.texts_to_sequences(data_df_dev['tweet'])            #convert dev text to integer sequences\n",
        "            train_X = pad_sequences(train_X, maxlen=max_len)                      #pad train sequences \n",
        "            dev_X = pad_sequences(dev_X, maxlen=max_len)\n",
        "            train_y=Y_target_train.values                                         #get target train class label values\n",
        "            \n",
        "            dev_y=Y_target_dev.values                                             \n",
        "\n",
        "            embedding_matrix=embedding_matrix_creation(tokenizer.word_index)      #create embedding matrix from glove\n",
        "\n",
        "            n_epochs = 6    \n",
        "            model = LSTM_NN()  \n",
        "            loss_function = nn.CrossEntropyLoss(reduction='sum')                  #setting loss fn\n",
        "            optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.003)         #setting Adam optimizer\n",
        "            \n",
        "            x_train = torch.tensor(train_X, dtype=torch.long)                     #tensor for tweets of training data\n",
        "            y_train = torch.tensor(train_y, dtype=torch.long)                     #tensor for sentiment of training data        \n",
        "            \n",
        "            x_dev = torch.tensor(dev_X, dtype=torch.long)                         #tensor for tweets of dev data\n",
        "            y_dev = torch.tensor(dev_y, dtype=torch.long)                         #tensor for sentiment of dev data   \n",
        "\n",
        "            train = torch.utils.data.TensorDataset(x_train, y_train)               # Create torch datasets    \n",
        "            valid = torch.utils.data.TensorDataset(x_dev, y_dev)\n",
        "           \n",
        "            train_loader = torch.utils.data.DataLoader(train, batch_size=1024, shuffle=True)       # Create data loaders for training data\n",
        "            valid_loader = torch.utils.data.DataLoader(valid, batch_size=1024, shuffle=False)       # Create data loaders for validation data\n",
        "\n",
        "            train_loss = []\n",
        "            valid_loss = []\n",
        "\n",
        "            for epoch in range(n_epochs):\n",
        "                start_time = time.time()\n",
        "               \n",
        "                model.train()                                                                     #training NN model\n",
        "                loss_computed = 0.                   \n",
        "                \n",
        "                for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "                                                                                            \n",
        "                    y_pred = model(x_batch)                                                        # run forward pass to train the model                 \n",
        "                    loss = loss_function(y_pred, y_batch)                                          # Compute loss\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.sum().backward()\n",
        "                    optimizer.step()\n",
        "                    loss_computed += loss.item() / len(train_loader)        \n",
        "\n",
        "                model.eval()                                                                        #evaluate loss for dev set\n",
        "                avg_val_loss = 0.\n",
        "                val_preds = np.zeros((len(x_dev),3))                                 \n",
        "\n",
        "                for i, (x_batch, y_batch) in enumerate(valid_loader):                           \n",
        "                    y_pred = model(x_batch).detach()                                               #run predictions on dev set\n",
        "                    avg_val_loss += loss_function(y_pred, y_batch).item() / len(valid_loader)            #compute loss\n",
        "                    val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred).cpu().numpy()  #running softmax function on predictions\n",
        "\n",
        "                val_accuracy = sum(val_preds.argmax(axis=1)==dev_y)/len(dev_y)                      #accuracy on dev set\n",
        "                train_loss.append(loss_computed)                                                           \n",
        "                valid_loss.append(avg_val_loss)                                                     \n",
        "                elapsed_time = time.time() - start_time \n",
        "                print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(\n",
        "                epoch + 1, n_epochs, loss_computed, avg_val_loss, val_accuracy, elapsed_time))\n",
        "           \n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        #Prediction performance of the classifiers\n",
        "        l=0\n",
        "        for testset in testsets:\n",
        "            \n",
        "            id_preds = {}\n",
        "            l=l+1\n",
        "\n",
        "            if features=='tfidf' and neural_network==False:                      #running predictions on testsets for tf-idf features              \n",
        "              id_preds=predict(model_trained,tf_feat['tf_features_test'+str(l)],Y_test['Y_target_test'+str(l)])\n",
        "\n",
        "              testset_name = testset\n",
        "              testset_path = join('semeval-tweets', testset_name)\n",
        "              confusion(id_preds, testset_path, classifier)                      #generate confusion matrix\n",
        "              evaluate(id_preds, testset_path, features + '-' + classifier)      #get F1 score\n",
        "\n",
        "            elif features=='bow' and neural_network==False:                      #running predictions on testsets for bag-of-words features   \n",
        "              id_preds=predict(model_trained,bow_feat['bow_features_test'+str(l)],Y_test['Y_target_test'+str(l)])\n",
        "\n",
        "              testset_name = testset\n",
        "              testset_path = join('semeval-tweets', testset_name)\n",
        "              confusion(id_preds, testset_path, classifier)\n",
        "              evaluate(id_preds, testset_path, features + '-' + classifier)\n",
        "\n",
        "            else:                                                                #running predictions on testsets for NN\n",
        "              x= df_test['data_df_test'+str(l)]['tweet']                           \n",
        "              x = tokenizer.texts_to_sequences(x)                                #convert text to sequence\n",
        "              x = pad_sequences(x, maxlen=max_len)                               #pad sequence\n",
        "              \n",
        "              x = torch.tensor(x, dtype=torch.long)                           \n",
        "\n",
        "              pred = model(x).detach()                                           #run NN model to get predictions\n",
        "              pred = F.softmax(pred).cpu().numpy()                               #running softmax function on predictions                    \n",
        "\n",
        "              pred = pred.argmax(axis=1)                                          \n",
        "\n",
        "              test_label= df_test['data_df_test'+str(l)]['sentiment']            #get class label  \n",
        "              id_preds=create_dict_from_predictions(pred,test_label)\n",
        "              \n",
        "              testset_name = testset\n",
        "              testset_path = join('semeval-tweets', testset_name)\n",
        "              confusion(id_preds, testset_path, classifier)                      #confusion matrix\n",
        "              evaluate(id_preds, testset_path, features + '-' + classifier)      #F1 score \n",
        "                  \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}